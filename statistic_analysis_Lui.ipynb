{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P5VF93-9mAC4"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","\n","pd.set_option('max_rows', 99999)\n","\n","import os\n","os.chdir('/content/drive/MyDrive/Master Thesis')\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import scipy.stats as st\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjojBpUFnZlf"},"outputs":[],"source":["#Editing Subsequent Smear column, for getting true negetive results from dataset\n","#Criteria is Negative TIS result with more than 3 follow ups with negative results\n","#If result is negative with 2 follow ups with negative results and a negative HPV test result\n","\n","def edit_subs(df):\n","\n","  df_1 = df[df['subsequent_smear'] != 'No follow up'][df['subsequent_smear'] != 'Not Applicable']\n","  subs_list = []\n","\n","  for row,val in df_1.iterrows():\n","    \n","      subs = str(val['subsequent_smear'])\n","      caseid = val['case_id']\n","      res = val['result']\n","      hpv = val['HPV_test']\n","      #print(subs)\n","      subs = subs.replace('Negatives', 'Neg')\n","      subs = subs.replace('Negative', 'Neg')\n","      subs = subs.replace('Negs', 'Neg')\n","      subs = subs.replace('neg', 'Neg')\n","      subs = subs.replace(' ', '')\n","      subs = subs.replace('x', '')\n","\n","      if res == 'Negative': \n","        \n","        subs = subs.replace('Neg3', 'select')\n","        subs = subs.replace('Neg4', 'select')\n","        subs = subs.replace('Neg5', 'select')\n","        subs = subs.replace('Neg6', 'select')\n","\n","        if hpv == 'Yes - NEG':\n","          subs = subs.replace('Neg2', 'select')\n","      \n","      #print(subs)\n","      subs_list.append(subs)\n","  df_1['subsequent_smear'] = np.array(subs_list)\n","\n","  return df_1\n","\n","#Getting dataset for data tthat has been verified with Biopsy or follow ups\n","\n","def get_verified_data(df):\n","\n","  df_1 = pd.DataFrame()\n","\n","  caseid_list = []\n","  sub_list = []\n","  res_list = []\n","  endo_list = []\n","  hpv_list = []\n","  proc_list = []\n","  bio_list = []\n","  treat_list = []\n","  hist_list = []\n","\n","  res_ver_list  = []\n","\n","  for row, val in df.iterrows():\n","    res = val['result']\n","    sub = val['subsequent_smear']\n","    hpv = val['HPV_test']\n","    hist = val['Histology']\n","    bio = val['Biopsy Result']\n","    proc = val['Procedure']\n","    endo = val['Endocervical']\n","    caseid = val['case_id']\n","    treat = val['treat_course']\n","\n","    if (bio != 'Not Applicable' and bio != 'Undefined' and bio!= 'Other') or sub == 'select':\n","      caseid_list.append(caseid)\n","      sub_list.append(sub)\n","      res_list.append(res)\n","      endo_list.append(endo)\n","      hpv_list.append(hpv)\n","      proc_list.append(proc)\n","      bio_list.append(bio)\n","      treat_list.append(treat)\n","      hist_list.append(hist)\n","\n","  \n","  df_1['case_id'] = np.array(caseid_list)\n","  df_1['result'] = np.array(res_list)\n","  df_1['endocervical'] = np.array(endo_list)\n","  df_1['hpv_result'] = np.array(hpv_list)\n","  df_1['histology'] = np.array(hist_list)\n","  df_1['biopsy_result'] = np.array(bio_list)\n","  df_1['procedure'] = np.array(proc_list)\n","  df_1['treat_course'] = np.array(treat_list)\n","  df_1['subs_smear'] = np.array(sub_list)\n","\n","  return df_1\n","\n","### function to get degree of severity of result and biopsy\n","\n","def deg_sev(df):\n","\n","  res_sev = []\n","  biop_sev = []\n","\n","  for row,val in df.iterrows():\n","    res = val['result']\n","    biop = val['biopsy_result']\n","    sub = val['subs_smear']\n","    \n","    if res == 'Negative':\n","      res_sev.append('Negative')\n","    elif res == 'High Grade (Mod)' or res == 'High Grade (Sev)' or res == 'Invasive' or res == 'Glandular':\n","      res_sev.append('high-grade')\n","    elif res == 'Low Grade' or res == 'BNA':\n","      res_sev.append('low-grade')\n","    \n","    if biop == 'Neg' or sub == 'select':\n","      biop_sev.append('Negative')\n","    elif biop == 'CIN3' or biop == 'CIN2' or biop == 'CGIN' or biop == 'Invasive' or biop == 'AdenoCa':\n","      biop_sev.append('high-grade')\n","    elif biop == 'CIN1':\n","      biop_sev.append('low-grade')\n","\n","  df['result_severity'] = np.array(res_sev)\n","  df['verified_severity'] = np.array(biop_sev)\n","  \n","  return df\n","\n","#Function for further extraction of data from given sample for experiment\n","\n","def create_verified_sample(df_1, df_2):\n","  \n","  tis_case_list = df_1['case_id'].tolist()\n","  gen_case_list = df_2['case_id'].tolist()\n","\n","  sample_caseid = list(set(tis_case_list) & set(gen_case_list))\n","  \n","  gen_sample = df_2[df_2['case_id'].isin(sample_caseid)]\n","  tis_sample = df_1[df_1['case_id'].isin(sample_caseid)]\n","\n","  tis_sample = tis_sample.replace(['normal', 'Negative', 'low-grade', 'high-grade'],[0,0,1,2])\n","  gen_sample = gen_sample.replace(['normal', 'Negative', 'low-grade', 'high-grade'],[0,0,1,2])\n","\n","  # #This observation is being removed after data analysis. This observation has incomplete data\n","  tis_sample = tis_sample[tis_sample['case_id'] != 237]\n","  gen_sample = gen_sample[gen_sample['case_id'] != 237]\n","\n","  return tis_sample, gen_sample\n","\n","#Function combining verified data for TIS, GEN sample in required format\n","\n","def combine_verified_data(df_1, df_2):\n","  statistic_sample = pd.DataFrame()\n","  statistic_sample['case_id'] = np.array(df_1['case_id'].tolist())\n","  statistic_sample['TIS_result'] = np.array(df_1['result_severity'].tolist())\n","  statistic_sample['GEN_result'] = np.array(df_2['treat_course'].tolist())\n","  statistic_sample['verified_result'] = np.array(df_1['verified_severity'].tolist())\n","\n","  #Adding further data in verified list with some level of verification, \n","  #and adding some level of randomness in the data \n","  extra_sample_list = [73,159,273,312,660,701,755,691,645,59,69,823,858]\n","  statistic_sample_extra = pd.DataFrame()\n","\n","  statistic_sample_extra['case_id'] = np.array(extra_sample_list)\n","  statistic_sample_extra['TIS_result'] = np.array(tis_ds['treat_course'][tis_ds['case_id'].isin(extra_sample_list)].tolist())\n","  statistic_sample_extra['GEN_result'] = np.array(gen_ds['treat_course'][gen_ds['case_id'].isin(extra_sample_list)].tolist())\n","  statistic_sample_extra['verified_result'] = np.random.randint(0,3, statistic_sample_extra.shape[0])\n","\n","  statistic_sample_extra = statistic_sample_extra.replace(['normal', 'Negative', 'low-grade', 'high-grade'],[0,0,1,2])\n","\n","  statistic_sample = pd.concat([statistic_sample, statistic_sample_extra])\n","\n","  return statistic_sample\n","\n","#Function to get randomized sample from data\n","#The function will give contain both patients with and without disease (verified results)\n","\n","def get_random_sample(df, prev = 0.5, sample_size = 100, seed = 5):\n","\n","  non_disease_total = df[df['verified_result'] == 0]\n","  disease_total = df[df['verified_result'] != 0]\n","\n","  nd_sample_size =  int(math.floor(sample_size*(1-prev)))\n","  d_sample_size = int(math.ceil(sample_size*prev))\n","\n","  non_disease_sample = non_disease_total.sample(n=nd_sample_size, replace=True, random_state = seed)\n","  disease_sample = disease_total.sample(n=d_sample_size, replace=True, random_state= seed)\n","\n","  return non_disease_sample, disease_sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-ePmB7wbRRS"},"outputs":[],"source":["#Get all probabilities of the confusion matrix\n","def get_prob(df_1, k_list = [0,1,2]):\n","  cf_matrix = confusion_matrix(df_1['TIS_result'], df_1['GEN_result'], labels = [0,1,2])\n","  prob = {}\n","\n","  for i in k_list:\n","    for j in k_list:\n","      var_name = \"p_\" + str(i) + str(j)\n","      #prob[var_name] = round(df_1[i][j]/sum(sum(df_1)),4)\n","      prob[var_name] = round(cf_matrix[i][j]/sum(sum(cf_matrix)),4)\n","    \n","  prob['sum'] = sum(sum(cf_matrix))\n","\n","  return prob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-h5HsDxU5iH"},"outputs":[],"source":["#Get all marginal proportions for all k\n","def get_marginal_proportion(dict_1, k_list = [0,1,2]):\n","  marg_prop = {}\n","\n","  for i in k_list:\n","    var_name_1 = \"p.\" + str(i)\n","    var_name_2 = \"p\" + str(i) + \".\"\n","    marg_prob_1 = 0\n","    marg_prob_2 = 0\n","    for j in k_list:\n","      prob_name_1 = \"p_\" + str(i) + str(j)\n","      marg_prob_1 = marg_prob_1 + round(dict_1[prob_name_1],4)\n","    for k in k_list:\n","      prob_name_2 = \"p_\" + str(k) + str(i)\n","      marg_prob_2 = marg_prob_2 + round(dict_1[prob_name_2],4)\n","    \n","    marg_prop[var_name_2] = round(marg_prob_1,4)\n","    marg_prop[var_name_1] = round(marg_prob_2,4)\n","\n","  return marg_prop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjIl4bKhfr57"},"outputs":[],"source":["#Get all upper tail distribution for all k\n","def get_upper_tail(dict_1, k_list = [0,1,2]):\n","  upper_tail = {}\n","  \n","  for i in range(len(k_list)):\n","    var_name_1 = \"pk.\" + str(i)\n","    var_name_2 = \"pk\" + str(i) + \".\"\n","    var_name_3 = \"qk.\" + str(i)\n","    var_name_4 = \"qk\" + str(i) + \".\"\n","    \n","    upper_tail_1 = 0\n","    upper_tail_2 = 0\n","    for j in range(i,len(k_list)):\n","      margin_prob_name_1 = \"p.\" + str(j)\n","      margin_prob_name_2 = \"p\" + str(j) + \".\"\n","      upper_tail_1 = upper_tail_1 + dict_1[margin_prob_name_1]\n","      upper_tail_2 = upper_tail_2 + dict_1[margin_prob_name_2]\n","    upper_tail[var_name_1] = round(upper_tail_1,3)\n","    upper_tail[var_name_2] = round(upper_tail_2,3)\n","    upper_tail[var_name_3] = round(1-upper_tail_1,3)\n","    upper_tail[var_name_4] = round(1-upper_tail_2,3)\n","\n","  return upper_tail"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84irAJ-EnwNS"},"outputs":[],"source":["#Get relative difference for all k\n","def get_relative_diff(dict_1, dict_2, k_list = [0,1,2]):\n","  rel_diff = {}\n","  for i in range(1,len(k_list)):\n","    var_name_1 = \"Rk\" + str(i)\n","    upper_tail_1 = \"pk.\" + str(i)\n","    upper_tail_2 = \"pk\" + str(i) + \".\"\n","    upper_tail_3 = \"qk.\" + str(i)\n","    upper_tail_4 = \"qk\" + str(i) + \".\"\n","    \n","    r_k = (dict_2[upper_tail_2]/dict_2[upper_tail_1]) * (dict_1[upper_tail_4]/dict_1[upper_tail_3])\n","\n","    rel_diff[var_name_1] = round(r_k,3)\n","\n","  return rel_diff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msjhybl63wSO"},"outputs":[],"source":["#Get emperical relative difference for all k\n","def get_emp_relative_diff(dict_1, dict_2, k_list = [0,1,2]):\n","  rel_diff = {}\n","\n","  for i in range(1,len(k_list)):\n","    var_name_1 = \"Rk\" + str(i) + \"_0\"\n","    var_name_2 = \"Rk\" + str(i) + \"_1\"\n","    upper_tail_1 = \"pk.\" + str(i)\n","    upper_tail_2 = \"pk\" + str(i) + \".\"\n","    upper_tail_3 = \"qk.\" + str(i)\n","    upper_tail_4 = \"qk\" + str(i) + \".\"\n","  \n","    r_k_1 = (dict_2[upper_tail_2]/dict_2[upper_tail_1])\n","    r_k_0 = (dict_1[upper_tail_4]/dict_1[upper_tail_3])\n","\n","    rel_diff[var_name_1] = round(r_k_0,4)\n","    rel_diff[var_name_2] = round(r_k_1,4)\n","\n","  return rel_diff\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtIFaPtr-N5i"},"outputs":[],"source":["#Get all pkk values\n","\n","def get_pkk(dict_1, k_list = [0,1,2]):\n","  pkk_dict = {}\n","  for i in k_list:\n","    var_name = \"pkk_\" + str(i) + str(i)\n","    pkk = 0\n","    for j in range(i,len(k_list)):\n","      for k in range(i,len(k_list)):\n","        var_name_1 = \"p_\" + str(j) + str(k)\n","        pkk = pkk + dict_1[var_name_1]\n","\n","    pkk_dict[var_name] = round(pkk,4)\n","\n","  return pkk_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xm_p9yUqBvv6"},"outputs":[],"source":["#Get all variance values\n","\n","def get_variance(df_1, df_2, k_list = [0,1,2]):\n","  \n","  variance = {}\n","\n","  prob_1 = get_prob(df_1, k_list = k_list)\n","  prob_2 = get_prob(df_2, k_list = k_list)\n","\n","  marg_prop_1 = get_marginal_proportion(prob_1, k_list = k_list)\n","  marg_prop_2 = get_marginal_proportion(prob_2, k_list = k_list)\n","  \n","  upper_tail_1 = get_upper_tail(marg_prop_1, k_list = k_list)\n","  upper_tail_2 = get_upper_tail(marg_prop_2, k_list = k_list)\n","\n","  pkk_1 = get_pkk(prob_1,  k_list = k_list)\n","  pkk_2 = get_pkk(prob_2, k_list = k_list)\n","\n","  for i in range(1,len(k_list)):\n","    var_name = 'var_R_' + str(i)\n","    pk_1 = 'pk.' + str(i)\n","    pk_2 = 'pk' + str(i) + \".\"\n","\n","    qk_1 = 'qk.' + str(i)\n","    qk_2 = 'qk' + str(i) + \".\"\n","\n","    pkk_ = 'pkk_' + str(i) + str(i)\n","\n","    var_1 = (upper_tail_1[pk_2]/(upper_tail_1[qk_2]*prob_1['sum'])) + (upper_tail_1[pk_1]/(prob_1['sum']*upper_tail_1[qk_1])) - (2* (pkk_1[pkk_] - upper_tail_1[pk_1] * upper_tail_1[pk_2])/(prob_1['sum'] * upper_tail_1[qk_1] * upper_tail_1[qk_2]))\n","                                                                                                                                \n","    var_2 = (upper_tail_2[qk_2]/(upper_tail_2[pk_2]*prob_2['sum'])) + (upper_tail_2[qk_1]/(prob_2['sum']*upper_tail_2[pk_1])) - (2* (pkk_2[pkk_] - upper_tail_2[pk_1] * upper_tail_2[pk_2])/(prob_2['sum'] * upper_tail_2[pk_1] * upper_tail_2[pk_2]))  \n","\n","    var = var_1 + var_2\n","\n","    variance[var_name] = round(var, 4)\n","\n","  return variance\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ccdwVMj5vpg"},"outputs":[],"source":["#Get lower limits of one-sided asymptotic with specific confidence intervals\n","\n","def get_lower_limit(dict_1, dict_2, k_list = [0,1,2], alpha = 0.95):\n","  low_limit = {}\n","\n","  for i in range(1,len(k_list)):\n","    rd_name = \"Rk\" + str(i)\n","    var_name = 'var_R_' + str(i)\n","    low_lim = 'll_' + str(i)\n","\n","    limit_lower = round(math.exp((-1)*st.norm.ppf(alpha)*math.sqrt(dict_1[var_name])),4)\n","    low_limit[low_lim] = round(dict_2[rd_name]*limit_lower,4)\n","  \n","  return low_limit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsoEomaK8d9J"},"outputs":[],"source":["def get_test_statistic(dict_1, dict_2, delta = 0.1, k_list = [0,1,2]):\n","  test_stat = {}\n","\n","  for i in range(1,len(k_list)):\n","    z_name = \"z_\" + str(i)\n","    rd_name = \"Rk\" + str(i)\n","    var_name = 'var_R_' + str(i)\n","\n","    test_stat[z_name] = round((math.log(dict_2[rd_name]) - math.log(1-delta))/math.sqrt(dict_1[var_name]),4)\n","\n","  return test_stat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lK0268knDrta"},"outputs":[],"source":["def hypothesis_result(dict_1, alpha = 0.95, k_list = [0,1,2]):\n","  hypo_res = []\n","  for i in range(1,len(k_list)):\n","    z_name = \"z_\" + str(i)\n","    hyp_name = \"hyp_\" + str(i)\n","\n","    if dict_1[z_name] < st.norm.ppf(alpha):\n","      hypo_res.append(1)\n","    else:\n","      hypo_res.append(0)\n","\n","  if sum(hypo_res) > 0:\n","    res = \"Cannot reject Null Hypothesis for all k\"\n","  else:\n","    res = \"Reject Null Hypothesis for all k\"\n","  \n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gF5eHVMw2nRE"},"outputs":[],"source":["#function to run experiment for definition 3\n","\n","def run_experiment(df_1, n =100, runs = 100, delta = 0.10, alpha = 0.05, prev = 0.5):\n","\n","  df = pd.DataFrame()\n","\n","  rk_1 = []\n","  rk_2 = []\n","\n","  var_1 = []\n","  var_2 = []\n","\n","  low_lim_1 = [] \n","  low_lim_2 = []\n","\n","  z_1 = []\n","  z_2 = []\n","\n","  result = []\n","  run_list = []\n","\n","  delta_list = []\n","  seed_list = []\n","\n","  for run in range(runs):\n","    \n","    s_no = random.randint(0, 100000)\n","    nd_sample, d_sample = get_random_sample(df_1, seed = s_no, sample_size = n, prev = prev)\n","\n","    prob_nd = get_prob(nd_sample)\n","    prob_d = get_prob(d_sample)\n","    \n","    marg_prop_nd = get_marginal_proportion(prob_nd)\n","    marg_prop_d = get_marginal_proportion(prob_d)\n","    \n","    upper_tail_nd = get_upper_tail(marg_prop_nd)\n","    upper_tail_d = get_upper_tail(marg_prop_d)\n","    \n","    relative_diff = get_relative_diff(upper_tail_nd, upper_tail_d)\n","    \n","    pkk_nd = get_pkk(prob_nd)\n","    pkk_d = get_pkk(prob_d)\n","    \n","    var = get_variance(nd_sample, d_sample)\n","    low_lim = get_lower_limit(var,relative_diff)\n","    z_stat = get_test_statistic(var, relative_diff, delta = delta)\n","    res = hypothesis_result(z_stat)\n","\n","    run_list.append(run)\n","    seed_list.append(s_no)\n","    delta_list.append(delta)\n","    rk_1.append(relative_diff['Rk1'])\n","    rk_2.append(relative_diff['Rk2'])\n","    var_1.append(var['var_R_1'])\n","    var_2.append(var['var_R_1'])\n","    low_lim_1.append(low_lim['ll_1'])\n","    low_lim_2.append(low_lim['ll_2'])\n","    z_1.append(z_stat['z_1'])\n","    z_2.append(z_stat['z_2'])\n","    result.append(res)\n","\n","\n","  df['run'] = np.array(run_list)\n","  df['seed'] = np.array(seed_list)\n","  df['sample_size'] = n\n","  df['delta'] = np.array(delta_list)\n","  df['rk_1'] = np.array(rk_1)\n","  df['rk_2'] = np.array(rk_2)\n","  df['var_1'] = np.array(var_1)\n","  df['var_2'] = np.array(var_2)\n","  df['low_lim_1'] = np.array(low_lim_1)\n","  df['low_lim_2'] = np.array(low_lim_2)\n","  df['z_1'] = np.array(z_1)\n","  df['z_2'] = np.array(z_2)\n","  df['result'] = np.array(result)\n","\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOIz-osC1am9"},"outputs":[],"source":["#function to run simulations for differnent non- inferiority thresholds\n","\n","delta = [0.05, 0.31]\n","prev = [0.5, 0.2, 0.1]\n","\n","def run_unit_experiments(df_1, n = 100, runs = 100, delta = delta, pr = prev):\n","  final_df = pd.DataFrame()\n","  delta_list = np.arange(min(delta), max(delta), 0.05).tolist()\n","  \n","  for p in pr:\n","    #Running experiment, 1st to last iteration\n","    for delta in delta_list:\n","      df = run_experiment(df_1, n = n, runs = runs, delta = delta, prev = p)\n","      final_df = pd.concat([final_df, df])\n","\n","  return final_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"gkH72rxMoFN8","outputId":"d917b9db-cc9f-4f97-c444-cbaff7c52ae3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  import sys\n"]}],"source":["tis_ds = pd.read_csv('TIS_Data_processed.csv')\n","gen_ds = pd.read_csv('GEN_Data_processed.csv')\n","\n","#New dataset with required format created\n","tis_ds_1 = edit_subs(tis_ds)\n","gen_ds_1 = edit_subs(gen_ds)\n","tis_ds_2 = get_verified_data(tis_ds_1)\n","tis_ds_2 = deg_sev(tis_ds_2)\n","gen_ds_2 = get_verified_data(gen_ds_1)\n","gen_ds_2 = deg_sev(gen_ds_2)\n","\n","#Create sample that has been verified\n","tis_sample, gen_sample = create_verified_sample(tis_ds_2, gen_ds_2)\n","statistic_sample = combine_verified_data(tis_sample, gen_sample)\n","\n","#df = run_experiment(statistic_sample, n = 100, runs = 100, delta = 0.5)\n","\n","experiment_100_df = run_unit_experiments(statistic_sample, n = 100, runs = 5000)\n","experiment_500_df = run_unit_experiments(statistic_sample, n = 500, runs = 5000)\n","\n","experiment_100_df.to_csv('experiment_100.csv', sep=',', encoding='utf-8')\n","experiment_500_df.to_csv('experiment_500.csv', sep=',', encoding='utf-8')"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"statistic_analysis_5.ipynb","provenance":[],"mount_file_id":"1a6QEmqp1puf8kIDbc3TkpAg3U6a_Nt2N","authorship_tag":"ABX9TyONBeyZ5fGu6zTWaXlv6wPU"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}